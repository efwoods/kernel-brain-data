{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach:\n",
    "\n",
    "    Preprocess Images: Convert them to grayscale, normalize, and subtract the baseline from the active image.\n",
    "    Segment Active Regions: Use OpenCV or a deep learning model to detect the highlighted brain regions.\n",
    "    Identify the Region: Use a pre-trained model (e.g., a convolutional neural network trained on brain MRI scans) or a simple region-based lookup to determine which part of the brain is active.\n",
    "    Overlay Annotations: Draw text labels or bounding boxes on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Settings, & Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "import librosa\n",
    "from IPython.display import Audio as IPDAudio\n",
    "from IPython.display import display, Image, Video\n",
    "from transformers import pipeline\n",
    "from datasets import Audio as Datasets_Audio\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2\n",
    "import time\n",
    "from glob import glob\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image as PIL_Image\n",
    "import shutil\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_file_path, audio_output_path):\n",
    "    \"\"\"\n",
    "    This function will accept a video file path and output\n",
    "    the extracted audio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_file_path : string\n",
    "        This is the path to the mp4 video.\n",
    "    audio_output_path : string\n",
    "        This is the path to the output audio wav file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_file_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract MFCC features\n",
    "def extract_mfcc(audio_path, sr=22050, n_mfcc=40, max_len=50):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Pad or truncate to max_len\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state_dict\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN Model\n",
    "class NeuralDataExpert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralDataExpert, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 2)  # Binary classification (Laughter vs. Non-Laughter)\n",
    "        self.brain_modality_mean = [17.6743, 17.4406, 17.1653]\n",
    "        self.brain_modality_std = [7.7531, 8.4884, 9.5815]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.brain_modality_mean, std=self.brain_modality_std)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, n_mfcc=40, max_len=96):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Compute the output size after pooling\n",
    "        fc_input_size = 32 * (n_mfcc // 4) * (max_len // 4)  # 2 pooling layers divide by 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_model(model_file_path, modelClass):\n",
    "    \"\"\"\n",
    "    This will return the loaded model using the \n",
    "    file path. It will use cuda if available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_file_path : str\n",
    "        This is the path of the stored pytorch model.\n",
    "    model : nn.Module class\n",
    "        This is the model definition used to define\n",
    "        the model that is loaded.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = modelClass()\n",
    "    model.load_state_dict(torch.load(model_file_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Video\n",
    "video_file_path = '../data/kernel-brain-data-jokes-lex-only-training-set.mp4'\n",
    "audio_file_path = '../data/kernel-brain-data-jokes-lex-only-training-set.wav'\n",
    "\n",
    "# Laughter Training Samples\n",
    "video_file_path_laughter_train = '../data/laughter-only/kernel-brain-data-jokes-lex-only-train-laughter-only.mp4'\n",
    "audio_file_path_laughter_train = '../data/laughter-only/kernel-brain-data-jokes-lex-only-train-laughter-only.wav'\n",
    "\n",
    "# Non-Laughter Training Samples\n",
    "video_file_path_non_laughter_train = '../data/non-laughter/kernel-brain-data-jokes-lex-only-training-set-non-laughter.mp4'\n",
    "audio_file_path_non_laughter_train = '../data/non-laughter/kernel-brain-data-jokes-lex-only-training-set-non-laughter.wav'\n",
    "\n",
    "# All Frames\n",
    "# laughter_all_frames = np.load('../data/laughter-only/laughter_all_frames.npy')\n",
    "# non_laughter_all_frames = np.load('../data/non-laughter/non_laughter_all_frames.npy')\n",
    "\n",
    "# Separated aughter Only\n",
    "# lex_human_laughter_only_np = np.load('../data/laughter-only/lex_human_laughter_only.npy')\n",
    "# lex_brain_laughter_only_np = np.load(\"../data/laughter-only/lex_brain_laughter_only.npy\")\n",
    "\n",
    "# Separated Non-Laughter\n",
    "# lex_human_non_laughter_np = np.load('../data/non-laughter/lex_human_non_laughter.npy')\n",
    "# lex_brain_non_laughter_np = np.load('../data/non-laughter/lex_brain_non_laughter.npy')\n",
    "\n",
    "# Data by modality\n",
    "brain_data_path = '../data/modalities/brain/'\n",
    "audio_data_path = '../data/modalities/audio/'\n",
    "human_data_path = '../data/modalities/human/'\n",
    "\n",
    "# Trained & Tested Models\n",
    "neural_data_model_f_path = '../models/neural_image_classifier_model.pth'\n",
    "audio_laughter_classifier_model_path = '../models/audio_classifier_model.pth'\n",
    "neural_data_model = load_torch_model(neural_data_model_f_path, NeuralDataExpert)\n",
    "audio_laughter_classifier_model = load_torch_model(audio_laughter_classifier_model_path, AudioClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Video Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sample_rate = librosa.load(audio_file_path)\n",
    "IPDAudio(data=audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = int(total_frames / fps)  # Duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract one frame per second\n",
    "frames = []\n",
    "for sec in tqdm(range(duration), desc=\"Extracting one frame per second...\", ascii=\"░▒▓█\"):\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)  # Move to the timestamp (sec * 1000ms)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "    frames.append(frame)\n",
    "\n",
    "cap.release()  # Release the video file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all frames\n",
    "index += 1 \n",
    "index = index % len(frames)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(frames[index])\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laughter Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_file_path_laughter_train)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = int(total_frames / fps)  # Duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_all_frames = []\n",
    "progress_bar = tqdm(total=cap.get(cv2.CAP_PROP_FRAME_COUNT), desc=\"Processing Frames\", unit=\"frame\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    laughter_all_frames.append(frame)\n",
    "    progress_bar.update(1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sample_rate = librosa.load(audio_file_path_laughter_train)\n",
    "IPDAudio(data=audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Neural Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index += 1\n",
    "index %= len(laughter_all_frames)\n",
    "plt.imshow(cv2.cvtColor(laughter_all_frames[index], cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_all_frames[index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_brain_regions = {\"lex_brain_y_t\":600}\n",
    "lex_brain_regions[\"lex_brain_y_b\"] = 1400\n",
    "lex_brain_regions[\"lex_brain_x_l\"] = 1900\n",
    "lex_brain_regions[\"lex_brain_x_r\"] = 2950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_human_regions = {}\n",
    "lex_human_regions[\"lex_human_y_t\"] = 0\n",
    "lex_human_regions[\"lex_human_y_b\"] = 1440\n",
    "lex_human_regions[\"lex_human_x_l\"] = 500\n",
    "lex_human_regions[\"lex_human_x_r\"] = 1800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(laughter_all_frames[0][:, lex_human_regions[\"lex_human_x_l\"]:lex_human_regions[\"lex_human_x_r\"], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Lex Human Laughter Only\n",
    "lex_human_laughter_only = []\n",
    "for frame in tqdm(range(len(laughter_all_frames)), desc=\"Extracting Lex Only Laughter Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_human_laughter_only.append(laughter_all_frames[frame][:, lex_human_regions[\"lex_human_x_l\"]:lex_human_regions[\"lex_human_x_r\"], :])\n",
    "lex_human_laughter_only_np = np.array(lex_human_laughter_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/laughter-only/lex_human_laughter_only.npy', lex_human_laughter_only_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(laughter_all_frames[0][600:1400, 1900:2950, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Lex Brain Laughter Only\n",
    "lex_brain_laughter_only = []\n",
    "for frame in tqdm(range(len(laughter_all_frames)), desc=\"Extracting Brain Only Laughter Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_brain_laughter_only.append(laughter_all_frames[frame][lex_brain_regions[\"lex_brain_y_t\"]:lex_brain_regions[\"lex_brain_y_b\"], lex_brain_regions[\"lex_brain_x_l\"]:lex_brain_regions[\"lex_brain_x_r\"], :])\n",
    "lex_brain_laughter_only_np = np.array(lex_brain_laughter_only)\n",
    "np.save(\"../data/laughter-only/lex_brain_laughter_only.npy\", lex_brain_laughter_only_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Laughter Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path_non_laughter_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sample_rate = librosa.load(audio_file_path_non_laughter_train)\n",
    "IPDAudio(data=audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_file_path_non_laughter_train)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = int(total_frames / fps)  # Duration in seconds\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_laughter_all_frames = []\n",
    "progress_bar = tqdm(total=cap.get(cv2.CAP_PROP_FRAME_COUNT), desc=\"Processing Frames\", unit=\"frame\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    non_laughter_all_frames.append(frame)\n",
    "    progress_bar.update(1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../data/non-laughter/non_laughter_all_frames.npy', np.array(non_laughter_all_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(laughter_all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_laughter_all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Lex Human Non Laughter\n",
    "lex_human_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_human_non_laughter.append(non_laughter_all_frames[frame][:, lex_human_regions[\"lex_human_x_l\"]:lex_human_regions[\"lex_human_x_r\"], :])\n",
    "lex_human_non_laughter_np = np.array(lex_human_non_laughter)\n",
    "np.save('../data/non-laughter/lex_human_non_laughter.npy', lex_human_non_laughter_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_brain_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Brain Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_brain_non_laughter.append(non_laughter_all_frames[frame][lex_brain_regions[\"lex_brain_y_t\"]:lex_brain_regions[\"lex_brain_y_b\"], lex_brain_regions[\"lex_brain_x_l\"]:lex_brain_regions[\"lex_brain_x_r\"], :])\n",
    "lex_brain_non_laughter_np = np.array(lex_brain_non_laughter)\n",
    "# np.save('../data/non-laughter/lex_brain_non_laughter.npy', lex_brain_non_laughter_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Neural Data Frame (Non-Laughter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Lex Human Non Laughter\n",
    "lex_human_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_human_non_laughter.append(non_laughter_all_frames[frame][:, lex_human_regions[\"lex_human_x_l\"]:lex_human_regions[\"lex_human_x_r\"], :])\n",
    "lex_human_non_laughter_np = np.array(lex_human_non_laughter)\n",
    "# np.save('../data/non-laughter/lex_human_non_laughter.npy', lex_human_non_laughter_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_brain_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Brain Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_brain_non_laughter.append(non_laughter_all_frames[frame][lex_brain_regions[\"lex_brain_y_t\"]:lex_brain_regions[\"lex_brain_y_b\"], lex_brain_regions[\"lex_brain_x_l\"]:lex_brain_regions[\"lex_brain_x_r\"], :])\n",
    "lex_brain_non_laughter_np = np.array(lex_brain_non_laughter)\n",
    "np.save('../data/non-laughter/lex_brain_non_laughter.npy', lex_brain_non_laughter_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Non-Laughter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path = \"../data/lex-attempting-not-to-laugh-no-audible-laughter.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = int(total_frames / fps)  # Duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_laughter_all_frames = []\n",
    "progress_bar = tqdm(total=cap.get(cv2.CAP_PROP_FRAME_COUNT), desc=\"Processing Frames\", unit=\"frame\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    non_laughter_all_frames.append(frame)\n",
    "    progress_bar.update(1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Lex Human Non Laughter\n",
    "lex_human_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_human_non_laughter.append(non_laughter_all_frames[frame][:, lex_human_regions[\"lex_human_x_l\"]:lex_human_regions[\"lex_human_x_r\"], :])\n",
    "lex_human_non_laughter_np = np.array(lex_human_non_laughter)\n",
    "# np.save('../data/non-laughter/lex_human_non_laughter.npy', lex_human_non_laughter_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_brain_non_laughter = []\n",
    "for frame in tqdm(range(len(non_laughter_all_frames)), desc=\"Extracting Lex Only Non Laughter Brain Frames...\", ascii=\"░▒▓█\"):\n",
    "    lex_brain_non_laughter.append(non_laughter_all_frames[frame][lex_brain_regions[\"lex_brain_y_t\"]:lex_brain_regions[\"lex_brain_y_b\"], lex_brain_regions[\"lex_brain_x_l\"]:lex_brain_regions[\"lex_brain_x_r\"], :])\n",
    "lex_brain_non_laughter_np = np.array(lex_brain_non_laughter)\n",
    "# np.save('../data/non-laughter/lex_brain_non_laughter.npy', lex_brain_non_laughter_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Brain Dataset Non-Laughter\n",
    "f_dir = '../data/non-laughter/test/brain/'\n",
    "for index in tqdm(range(len(lex_brain_non_laughter_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "    f_name = f\"lex_brain_non_laughter_test_{index}.png\"\n",
    "    PIL_Image.fromarray(lex_brain_non_laughter_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Human Dataset Non-Laughter\n",
    "f_dir = '../data/non-laughter/test/human/'\n",
    "for index in tqdm(range(len(lex_human_non_laughter_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "    f_name = f\"lex_human_non_laughter_test_{index}.png\"\n",
    "    PIL_Image.fromarray(lex_human_non_laughter_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating Brain Dataset Laughter\n",
    "# f_dir = '../data/laughter-only/brain/'\n",
    "# for index in tqdm(range(len(lex_brain_laughter_only_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "#     f_name = f\"lex_brain_laughter_only_{index}.png\"\n",
    "#     PIL_Image.fromarray(lex_brain_laughter_only_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating Brain Dataset Non-Laughter\n",
    "# f_dir = '../data/non-laughter/brain/'\n",
    "# for index in tqdm(range(len(lex_brain_non_laughter_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "#     f_name = f\"lex_brain_non_laughter_{index}.png\"\n",
    "#     PIL_Image.fromarray(lex_brain_non_laughter_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating Human Dataset Laughter\n",
    "# f_dir = '../data/laughter-only/human/'\n",
    "# for index in tqdm(range(len(lex_human_laughter_only_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "#     f_name = f\"lex_human_laughter_only_{index}.png\"\n",
    "#     PIL_Image.fromarray(lex_human_laughter_only_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating Human Dataset Non-Laughter\n",
    "# f_dir = '../data/non-laughter/human/'\n",
    "# for index in tqdm(range(len(lex_human_non_laughter_np)), desc=\"Saving Frames as Images...\", ascii=\"░▒▓█\"):\n",
    "#     f_name = f\"lex_human_non_laughter_{index}.png\"\n",
    "#     PIL_Image.fromarray(lex_human_non_laughter_np[index].astype(np.uint8)).save(f_dir + f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting audio lecture segments\n",
    "smile_data_dir = '../data/smile/SMILE_DATASET_v2/SMILE_DATASET/videos/SMILE_video/video_segments/'\n",
    "non_laughter_audio_segment_file_paths = glob(smile_data_dir + \"*_*_*\")\n",
    "single_underscore_file_candidates = glob(smile_data_dir + '*_*')\n",
    "laughter_only_audio_segment_file_paths = [file for file in single_underscore_file_candidates if os.path.basename(file).count('_') == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(laughter_only_audio_segment_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(non_laughter_audio_segment_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder_laughter = '../data/modalities/audio/laughter-only-mp4'\n",
    "destination_folder_non_laughter = '../data/modalities/audio/non-laughter-mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_path in tqdm(non_laughter_audio_segment_file_paths, desc=\"Copying non-laughter audio segments...\", ascii=\"░▒▓█\"):\n",
    "#     # Get the filename from the file path\n",
    "#     filename = os.path.basename(file_path)\n",
    "    \n",
    "#     # Construct the destination file path\n",
    "#     destination_path = os.path.join(destination_folder_non_laughter, filename)\n",
    "    \n",
    "#     # Copy the file to the destination folder\n",
    "#     shutil.copy(file_path, destination_path)\n",
    "    \n",
    "#     print(f\"Copied {file_path} to {destination_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_path in tqdm(laughter_only_audio_segment_file_paths, desc=\"Copying laughter only audio segments...\", ascii=\"░▒▓█\"):\n",
    "#     # Get the filename from the file path\n",
    "#     filename = os.path.basename(file_path)\n",
    "    \n",
    "#     # Construct the destination file path\n",
    "#     destination_path = os.path.join(destination_folder_laughter, filename)\n",
    "    \n",
    "#     # Copy the file to the destination folder\n",
    "#     shutil.copy(file_path, destination_path)\n",
    "    \n",
    "#     # print(f\"Copied {file_path} to {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Emotion Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_human_laughter_only_gray_np_img = cv2.cvtColor(lex_human_laughter_only_np[0], cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(lex_human_laughter_only_gray_np_img, cmap=\"gray\")\n",
    "lex_human_laughter_only_gray_np_img.shape\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# plt.imshow(lex_human_laughter_only_gray_np[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Face Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    # Read and process image\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_image)\n",
    "\n",
    "    # Extract landmarks\n",
    "    landmarks = []\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x, y = landmark.x, landmark.y  # Normalized (0 to 1)\n",
    "                landmarks.append([x, y])\n",
    "\n",
    "    if len(landmarks) == 468:  # Ensure full face mesh is detected\n",
    "        return np.array(landmarks).flatten()  # Convert to 1D array\n",
    "    else:\n",
    "        return None  # Skip if face not detected\n",
    "\n",
    "# Example usage\n",
    "landmark_vector = extract_landmarks(\"laughter.jpg\")\n",
    "print(landmark_vector.shape)  # Should be (936,) if all landmarks are found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Emotion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class LaughterDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        landmarks = extract_landmarks(self.image_paths[idx])\n",
    "        if landmarks is None:\n",
    "            return None  # Skip if no face detected\n",
    "\n",
    "        landmarks = torch.tensor(landmarks, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return landmarks, label\n",
    "\n",
    "# Load dataset\n",
    "image_paths = [\"laugh1.jpg\", \"serious1.jpg\", \"laugh2.jpg\"]\n",
    "labels = [1, 0, 1]  # 1 = laughter, 0 = no laughter\n",
    "\n",
    "dataset = LaughterDataset(image_paths, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LaughterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaughterCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(936, 256)  # 468 x 2 = 936 input features\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Binary classification (0 = no laughter, 1 = laughter)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = LaughterCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Emotion Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_laughter(image_path, model):\n",
    "    landmarks = extract_landmarks(image_path)\n",
    "    if landmarks is None:\n",
    "        return \"No Face Detected\"\n",
    "    \n",
    "    landmarks = torch.tensor(landmarks, dtype=torch.float32).unsqueeze(0)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(landmarks)\n",
    "        prob = output.item()\n",
    "\n",
    "    return \"Laughter\" if prob > 0.5 else \"No Laughter\"\n",
    "\n",
    "print(predict_laughter(\"test_laugh.jpg\", model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Neural Image Expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lex_brain_laughter_only_np[0])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Number of Examples of Laughter: {len(lex_brain_laughter_only_np)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lex_brain_non_laughter_np[0])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Number of Examples of Non-Laughter: {len(lex_brain_non_laughter_np)}\")\n",
    "lex_brain_non_laughter_np[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained model\n",
    "\n",
    "# Load and preprocess the image\n",
    "# image_path = \"../data/lex-brain-laughter-01-27.png\"  # Path to your uploaded image\n",
    "# image_path = \"../data/lex-brain-baseline-00-37.png\"  # Path to your uploaded image\n",
    "\n",
    "test_image_path = glob('../data/non-laughter/test/brain/' + \"*.png\")\n",
    "\n",
    "# image_path = \"../data/lex-non-laughter.png\"  # Path to your uploaded image\n",
    "image_path  = test_image_path[index]\n",
    "\n",
    "image = PIL_Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB mode\n",
    "plt.imshow(cv2.imread(image_path))\n",
    "plt.title(f\"{os.path.basename(image_path)}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "index+=1 \n",
    "index%=len(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index += 1\n",
    "# index %= len(lex_brain_non_laughter_np)\n",
    "# image = lex_brain_non_laughter_np[index]\n",
    "# image = PIL_Image.fromarray(image.astype(np.uint8)).convert(\"RGB\")\n",
    "image = neural_data_model.transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# plt.imshow(lex_brain_non_laughter_np[index])\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# print(f\"Number of Examples of Non-Laughter: {len(lex_brain_non_laughter_np)}\")\n",
    "\n",
    "neural_data_model.eval()\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = neural_data_model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Interpret result\n",
    "classes = [\"Non-Laughter\", \"Laughter\"]\n",
    "print(f\"Predicted class: {classes[prediction]}\")\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Real-World Test Accuracy\n",
    "\n",
    "image_path  = test_image_path[index]\n",
    "neural_data_model.eval()\n",
    "test_truth = np.zeros(len(test_image_path))\n",
    "predictions = []\n",
    "for image_path in tqdm(test_image_path, desc=\"Calculating Test Accuracy...\", ascii=\"░▒▓█\"):\n",
    "    image = PIL_Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB mode\n",
    "    image = neural_data_model.transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = neural_data_model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "        predictions.append(prediction)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_truth, np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index += 1\n",
    "index %= len(lex_brain_laughter_only_np)\n",
    "image = lex_brain_laughter_only_np[index]\n",
    "image = PIL_Image.fromarray(image.astype(np.uint8)).convert(\"RGB\")\n",
    "image = neural_data_model.transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "plt.imshow(lex_brain_laughter_only_np[index])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Number of Examples of Non-Laughter: {len(lex_brain_laughter_only_np)}\")\n",
    "\n",
    "neural_data_model.eval()\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = neural_data_model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Interpret result\n",
    "classes = [\"Non-Laughter\", \"Laughter\"]\n",
    "print(f\"Predicted class: {classes[prediction]}\")\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Mean and Variance to Normalize the data:\n",
    "\n",
    "# Load your dataset\n",
    "dataset = tv_datasets.ImageFolder(root=brain_data_path, transform=transforms.ToTensor())\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize variables to compute the mean and std\n",
    "brain_modality_mean = 0.0\n",
    "brain_modality_std = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Iterate over the dataset to compute mean and std\n",
    "for images, _ in dataloader:\n",
    "    batch_samples = images.size(0)  # Get the number of images in the batch\n",
    "    images = images.view(batch_samples, images.size(1), -1)  # Flatten the images\n",
    "    brain_modality_mean += images.mean(2).sum(0)  # Sum the mean of each image channel\n",
    "    brain_modality_std += images.std(2).sum(0)    # Sum the std of each image channel\n",
    "    num_batches += 1\n",
    "\n",
    "brain_modality_mean /= num_batches  # Average mean over all batches\n",
    "brain_modality_std /= num_batches    # Average std over all batches\n",
    "\n",
    "print(\"Mean:\", brain_modality_mean)\n",
    "print(\"Std:\", brain_modality_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match model input size\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=brain_modality_mean, std=brain_modality_std)  # Normalize pixel values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Assuming laughter and non-laughter images are in separate folders)\n",
    "dataset = tv_datasets.ImageFolder(root=brain_data_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "neuralModel = NeuralDataExpert().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(neuralModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), desc=\"Training...\", ascii=\"░▒▓█\"):\n",
    "    neuralModel.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = neuralModel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "neuralModel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = neuralModel(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(neuralModel, \"../models/neural_image_classifier_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Laughter Expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Audio Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sr=22050, n_mfcc=40, max_len=50):\n",
    "        self.root_dir = root_dir\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.max_len = max_len\n",
    "        self.audio_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Assign labels based on folder names\n",
    "        self.label_map = {\"laughter-only\": 1, \"non-laughter\": 0}\n",
    "\n",
    "        for label_name in self.label_map.keys():\n",
    "            label_path = os.path.join(root_dir, label_name)\n",
    "            if os.path.isdir(label_path):\n",
    "                for file_name in os.listdir(label_path):\n",
    "                    if file_name.endswith(\".wav\"):\n",
    "                        file_path = os.path.join(label_path, file_name)\n",
    "                        self.audio_paths.append(file_path)\n",
    "                        self.labels.append(self.label_map[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = torch.tensor(float(self.labels[idx]), dtype=torch.float32)  # BCELoss needs float labels\n",
    "\n",
    "        # Extract MFCC features and convert to tensor\n",
    "        mfcc = extract_mfcc(audio_path, sr=self.sr, n_mfcc=self.n_mfcc, max_len=self.max_len)\n",
    "        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        return mfcc_tensor, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MFCC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/modalities/audio/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = AudioDataset(root_dir=audio_data_path, n_mfcc=40, max_len=96)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AudioClassifier(n_mfcc=40, max_len=96).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "dataset = AudioDataset(root_dir=audio_data_path, n_mfcc=40, max_len=96)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% for training and 20% for testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:54<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:55<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:57<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:57<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:56<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:55<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:57<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:56<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:56<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 270/270 [00:56<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set number of training epochs\n",
    "num_epochs = 10  # You can adjust this\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "# dataset = AudioDataset(root_dir=audio_data_path, n_mfcc=40, max_len=96)\n",
    "# train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AudioClassifier(n_mfcc=40, max_len=96).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for mfccs, labels in tqdm(train_loader, desc=\"Training...\", ascii=\"░▒▓█\"):\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Normalize MFCCs\n",
    "        mfccs = (mfccs - mfccs.mean()) / mfccs.std()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(mfccs)  # Ensure correct input shape\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = audio_laughter_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Predictions...: 100%|██████████| 54/54 [00:09<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for mfccs, labels in tqdm(test_loader, desc=\"Calculating Predictions...\", ascii=\"░▒▓█\"):\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        \n",
    "        # Normalize MFCCs\n",
    "        mfccs = (mfccs - mfccs.mean()) / mfccs.std()\n",
    "\n",
    "        # # Ensure MFCCs are correctly shaped\n",
    "        # mfccs = mfccs.unsqueeze(1)  # Add channel dimension if missing\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(mfccs)\n",
    "\n",
    "        # Convert model outputs to binary predictions (0 or 1)\n",
    "        predicted = (outputs >= 0.5).float()  # Sigmoid outputs >= 0.5 -> 1, else 0\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.view(-1) == labels).sum().item()  # Compare predictions\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "save_model(model, \"../models/audio_classifier_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_laughter_classifier_model = load_torch_model(audio_laughter_classifier_model_path, AudioClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Predictions...: 100%|██████████| 54/54 [00:10<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_laughter_classifier_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for mfccs, labels in tqdm(test_loader, desc=\"Calculating Predictions...\", ascii=\"░▒▓█\"):\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        \n",
    "        # Normalize MFCCs\n",
    "        mfccs = (mfccs - mfccs.mean()) / mfccs.std()\n",
    "\n",
    "        # # Ensure MFCCs are correctly shaped\n",
    "        # mfccs = mfccs.unsqueeze(1)  # Add channel dimension if missing\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = audio_laughter_classifier_model(mfccs)\n",
    "\n",
    "        # Convert model outputs to binary predictions (0 or 1)\n",
    "        predicted = (outputs >= 0.5).float()  # Sigmoid outputs >= 0.5 -> 1, else 0\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.view(-1) == labels).sum().item()  # Compare predictions\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: (40, 14016)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the audio file (assuming you have a .wav file)\n",
    "audio_file = \"../data/kernel-brain-data-audio.wav\"\n",
    "audio_data, fs = librosa.load(audio_file, sr=None)  # Load audio with original sampling rate\n",
    "\n",
    "# Define the parameters for MFCC extraction\n",
    "n_mfcc = 40  # Number of MFCCs\n",
    "hop_length = 512  # Hop length (frame shift)\n",
    "n_fft = 2048  # FFT window size\n",
    "\n",
    "# Compute MFCC features\n",
    "mfcc = librosa.feature.mfcc(y=audio_data, sr=fs, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "# Ensure that we have exactly 40 MFCCs and 96 frames per second\n",
    "# Check if mfcc has the right number of frames per second (96 frames per second)\n",
    "# Librosa will give us variable frames, so we need to pad or truncate if necessary.\n",
    "\n",
    "target_frames = 96  # Target number of frames per second\n",
    "\n",
    "# Adjust number of frames to match 96 frames per second (if necessary)\n",
    "frames_per_second = mfcc.shape[1] / (len(audio_data) / fs)\n",
    "desired_length = target_frames * (len(audio_data) // fs)\n",
    "\n",
    "# If we need to truncate or pad\n",
    "if mfcc.shape[1] > desired_length:\n",
    "    mfcc = mfcc[:, :desired_length]  # Truncate\n",
    "elif mfcc.shape[1] < desired_length:\n",
    "    pad_width = desired_length - mfcc.shape[1]\n",
    "    mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')  # Pad with zeros\n",
    "\n",
    "# Verify the shape\n",
    "print(f\"MFCC shape: {mfcc.shape}\")  # Expected shape: (40, 96*length_in_seconds)\n",
    "\n",
    "# Now, feed the data to the model 1 second at a time\n",
    "audio_chunks = []\n",
    "\n",
    "for i in range(0, mfcc.shape[1], target_frames):\n",
    "    chunk = mfcc[:, i:i+target_frames]  # Get 1-second chunk of MFCCs (40 x 96)\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 40, 96)\n",
    "    \n",
    "    # Store the chunk for feeding into the model\n",
    "    audio_chunks.append(chunk_tensor)\n",
    "\n",
    "# Now, you can use `audio_chunks` to feed into the model one chunk at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Audio...: 100%|██████████| 146/146 [00:00<00:00, 3141.82it/s]\n"
     ]
    }
   ],
   "source": [
    "classifications = []\n",
    "for chunk in tqdm(range(len(audio_chunks)), desc=\"Classifying Audio...\", ascii=\"░▒▓█\"):\n",
    "    pred = audio_laughter_classifier_model(audio_chunks[chunk].to(device))\n",
    "    classifications.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [1 if classifications[index][0] == 1 else 0 for index in range(len(classifications))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 146\n",
    "ground_truth = [7, 12, 63, 66, 77, 85, 86, 87, 88, 89, 90, 106, 107, 108, 109, 110]\n",
    "# Create a zero array\n",
    "ground_truth_bin = np.zeros(size, dtype=int)\n",
    "\n",
    "# Set specified indices to 1\n",
    "np.put(ground_truth_bin , ground_truth, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(classifications).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of audio classifier: 0.36301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of audio classifier: {accuracy_score(ground_truth_bin, classifications):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of Audio Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Load and play the audio\n",
    "audio_path = \"../data/kernel-brain-data-audio.wav\"\n",
    "wav_file = wave.open(audio_path, 'rb')\n",
    "fs = wav_file.getframerate()\n",
    "audio_data = np.frombuffer(wav_file.readframes(wav_file.getnframes()), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = int(wav_file.getnframes() / fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6eeffd97f00473fb8b22ad662e7d25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Starting...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "# Create text widget\n",
    "text_widget = widgets.Label(value=\"Starting...\")\n",
    "\n",
    "# Function to update text dynamically while playing audio\n",
    "def play_audio_with_text():\n",
    "    sd.play(audio_data, samplerate=fs*2)  # Play the audio\n",
    "\n",
    "    for sec in range(min(seconds, int(wav_file.getnframes() / fs))):\n",
    "        if ground_truth_bin[sec] == 1: # implementing human-labeling rather than the classifier\n",
    "        # pred = audio_laughter_classifier_model(audio_chunks[sec].to(device))\n",
    "        # if  pred[0][0] == 1:\n",
    "            text_widget.value = f\"Time: {sec}s \\nClassification: Laughter\"\n",
    "        else:\n",
    "            text_widget.value = f\"\"\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "\n",
    "    sd.stop()  # Stop playback when done\n",
    "\n",
    "# Display text widget\n",
    "display(text_widget)\n",
    "\n",
    "# Start playback and text update\n",
    "play_audio_with_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Pose Expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Pose Body Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "def extract_pose_landmarks(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_image)\n",
    "\n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            x, y, visibility = landmark.x, landmark.y, landmark.visibility\n",
    "            landmarks.append([x, y, visibility])\n",
    "\n",
    "    if len(landmarks) == 33:  # Ensure full pose is detected\n",
    "        return np.array(landmarks).flatten()  # Convert to 1D array\n",
    "    else:\n",
    "        return None  # Skip if pose not detected\n",
    "\n",
    "# Example usage\n",
    "pose_vector = extract_pose_landmarks(\"laughter_pose.jpg\")\n",
    "print(pose_vector.shape)  # Should be (99,) if all landmarks are found (33 x 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laughter Pose CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LaughterPoseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaughterPoseCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(99, 128)  # 33 x 3 = 99 input features\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Binary classification (0 = no laughter, 1 = laughter)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = LaughterPoseCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laughter Pose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoseLaughterDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        landmarks = extract_pose_landmarks(self.image_paths[idx])\n",
    "        if landmarks is None:\n",
    "            return None  # Skip if no pose detected\n",
    "\n",
    "        landmarks = torch.tensor(landmarks, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return landmarks, label\n",
    "\n",
    "# Load dataset\n",
    "image_paths = [\"laugh1.jpg\", \"serious1.jpg\", \"laugh2.jpg\"]\n",
    "labels = [1, 0, 1]  # 1 = laughter, 0 = no laughter\n",
    "\n",
    "dataset = PoseLaughterDataset(image_paths, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Laughter Pose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        if batch is None:\n",
    "            continue  # Skip invalid samples\n",
    "        \n",
    "        landmarks, labels = batch\n",
    "        labels = labels.unsqueeze(1)  # Reshape for BCELoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(landmarks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_laughter_from_pose(image_path, model):\n",
    "    landmarks = extract_pose_landmarks(image_path)\n",
    "    if landmarks is None:\n",
    "        return \"No Pose Detected\"\n",
    "    \n",
    "    landmarks = torch.tensor(landmarks, dtype=torch.float32).unsqueeze(0)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(landmarks)\n",
    "        prob = output.item()\n",
    "\n",
    "    return \"Laughter\" if prob > 0.5 else \"No Laughter\"\n",
    "\n",
    "print(predict_laughter_from_pose(\"test_laughter_pose.jpg\", model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Mixture of Experts Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Expert Networks (one for each modality)\n",
    "class AudioExpert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioExpert, self).__init__()\n",
    "        self.fc = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class FacialEmotionExpert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FacialEmotionExpert, self).__init__()\n",
    "        self.fc = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class BodyPoseExpert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BodyPoseExpert, self).__init__()\n",
    "        self.fc = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class NeuralDataExpert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralDataExpert, self).__init__()\n",
    "        self.fc = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Gating Network (Decides how much weight each expert gets)\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(1024, 4)  # 4 experts (one per modality)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_weights = F.softmax(self.fc(x), dim=1)  # Output weights for each expert\n",
    "        return gate_weights\n",
    "\n",
    "\n",
    "# MoE Model that uses Gating Network and Expert Networks\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.audio_expert = AudioExpert()\n",
    "        self.facial_emotion_expert = FacialEmotionExpert()\n",
    "        self.body_pose_expert = BodyPoseExpert()\n",
    "        self.neural_data_expert = NeuralDataExpert()\n",
    "        self.gating_network = GatingNetwork()\n",
    "        self.final_fc = nn.Linear(256, 1)\n",
    "        \n",
    "        # Final prediction layer for binary output (laughter: 1, not laughter: 0)\n",
    "\n",
    "    def forward(self, audio, facial_emotion, body_pose, neural_data):\n",
    "        # Concatenate inputs to pass through the gating network\n",
    "        combined_input = torch.cat(\n",
    "            (audio, facial_emotion, body_pose, neural_data), dim=1\n",
    "        )\n",
    "\n",
    "        # Get gate weights (importance of each modality)\n",
    "        gate_weights = self.gating_network(combined_input)\n",
    "\n",
    "        # Compute the output of each expert\n",
    "        audio_output = self.audio_expert(audio)\n",
    "        facial_emotion_output = self.facial_emotion_expert(facial_emotion)\n",
    "        body_pose_output = self.body_pose_expert(body_pose)\n",
    "        neural_data_output = self.neural_data_expert(neural_data)\n",
    "\n",
    "        # Combine the outputs weighted by the gate's output\n",
    "        weighted_outputs = (\n",
    "            gate_weights[:, 0].unsqueeze(1) * audio_output\n",
    "            + gate_weights[:, 1].unsqueeze(1) * facial_emotion_output\n",
    "            + gate_weights[:, 2].unsqueeze(1) * body_pose_output\n",
    "            + gate_weights[:, 3].unsqueeze(1) * neural_data_output\n",
    "        )\n",
    "\n",
    "        final_output = self.final_fc(weighted_outputs)\n",
    "\n",
    "        # Sigmoid activation to get probability\n",
    "        prediction = torch.sigmoid(final_output)\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = MixtureOfExperts()\n",
    "audio_data = torch.randn(1, 256)  # Example audio data\n",
    "facial_emotion_data = torch.randn(1, 256)  # Example facial emotion data\n",
    "body_pose_data = torch.randn(1, 256)  # Example body pose data\n",
    "neural_data = torch.randn(1, 256)  # Example neural data\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(audio_data, facial_emotion_data, body_pose_data, neural_data)\n",
    "print(f\"Predicted output: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LaughterDataset(Dataset):\n",
    "    def __init__(self, audio_features, facial_features, body_features, neural_features, labels):\n",
    "        self.audio_features = audio_features\n",
    "        self.facial_features = facial_features\n",
    "        self.body_features = body_features\n",
    "        self.neural_features = neural_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.audio_features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.facial_features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.body_features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.neural_features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation data\n",
    "import numpy as np\n",
    "\n",
    "# Load or generate random data (replace with actual data loading)\n",
    "num_samples = 1000  # Adjust based on dataset size\n",
    "audio_features = np.random.randn(num_samples, 256)\n",
    "facial_features = np.random.randn(num_samples, 256)\n",
    "body_features = np.random.randn(num_samples, 256)\n",
    "neural_features = np.random.randn(num_samples, 256)\n",
    "labels = np.random.randint(0, 2, size=(num_samples, 1))  # Binary labels\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size = num_samples - train_size\n",
    "\n",
    "dataset = LaughterDataset(audio_features, facial_features, body_features, neural_features, labels)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training loop\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MixtureOfExperts().to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for audio, facial, body, neural, label in train_loader:\n",
    "            audio, facial, body, neural, label = (\n",
    "                audio.to(device),\n",
    "                facial.to(device),\n",
    "                body.to(device),\n",
    "                neural.to(device),\n",
    "                label.to(device),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, facial, body, neural)\n",
    "            loss = criterion(output, label)  # Compute loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for audio, facial, body, neural, label in val_loader:\n",
    "                audio, facial, body, neural, label = (\n",
    "                    audio.to(device),\n",
    "                    facial.to(device),\n",
    "                    body.to(device),\n",
    "                    neural.to(device),\n",
    "                    label.to(device),\n",
    "                )\n",
    "\n",
    "                output = model(audio, facial, body, neural)\n",
    "                val_loss += criterion(output, label).item()\n",
    "                predicted = (output > 0.5).float()  # Convert probability to class (0 or 1)\n",
    "                correct += (predicted == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "# Train for 20 epochs\n",
    "train_model(model, train_loader, val_loader, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# torch.save(model.state_dict(), \"moe_laughter_model.pth\")\n",
    "\n",
    "# Loading the model\n",
    "# model = MixtureOfExperts()\n",
    "# model.load_state_dict(torch.load(\"moe_laughter_model.pth\"))\n",
    "# model.to(device)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "# Example input\n",
    "new_audio = torch.randn(1, 256).to(device)\n",
    "new_facial = torch.randn(1, 256).to(device)\n",
    "new_body = torch.randn(1, 256).to(device)\n",
    "new_neural = torch.randn(1, 256).to(device)\n",
    "\n",
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_audio, new_facial, new_body, new_neural)\n",
    "    predicted_label = (prediction > 0.5).float().item()  # Convert to class label\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")  # 1 for laughter, 0 for no laughter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Audio: Convert audio signals into spectrograms or MFCCs (Mel-Frequency Cepstral Coefficients).\n",
    "    Facial Emotion: Use a facial recognition system (like OpenCV or Dlib) to extract facial landmarks and classify emotional states.\n",
    "    Body Pose: Use a pose estimation model (like OpenPose or MediaPipe) to extract keypoints of the body.\n",
    "    Neural Data: Normalize brain activity (e.g., fMRI, EEG) to a standard scale.\n",
    "\n",
    "2. Model Architecture\n",
    "\n",
    "The core idea is to create a multi-encoder architecture with modality-specific subnetworks and a fusion layer. Here's how to structure it:\n",
    "Step 1: Modality-Specific Encoders\n",
    "\n",
    "Create separate neural networks (like CNNs, RNNs, or Transformers) for each modality. Here's how:\n",
    "\n",
    "    Audio Encoder: Use a 1D CNN or RNN to process audio features like spectrograms.\n",
    "    Facial Emotion Encoder: Use a CNN (ResNet, VGG) to process facial emotion features extracted from images.\n",
    "    Body Pose Encoder: Use a CNN or RNN to process body pose keypoints.\n",
    "    Neural Data Encoder: Use a simple MLP (Multi-layer Perceptron) or RNN to process neural data like EEG or fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_laughter_audio_data, non_laughter_sample_rate = librosa.load(audio_file_path_non_laughter_train)\n",
    "IPDAudio(data=non_laughter_audio_data, rate=non_laughter_sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "laughter_only_audio_data, laughter_only_sample_rate = librosa.load(audio_file_path_laughter_train)\n",
    "IPDAudio(data=laughter_only_audio_data, rate=laughter_only_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
